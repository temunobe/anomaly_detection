{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17f8eaf",
   "metadata": {},
   "source": [
    "# Structure\n",
    "1. Dependecies\n",
    "2. Model\n",
    "3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93fc96",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb92e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-25 09:31:46.509653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756132307.046530   34932 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756132307.141698   34932 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756132308.120947   34932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756132308.121015   34932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756132308.121019   34932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756132308.121022   34932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-25 09:31:48.144077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, torch, requests, logging, json, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset as HFDataset\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import RobertaTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddab33",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b59eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae870a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce16905",
   "metadata": {},
   "source": [
    "# Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad18fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = config['data_dir'] \n",
    "MODEL_NAME = \"roberta-base\"\n",
    "OUTPUT_DIR = config['output']\n",
    "LOGGING_DIR = config['logs'] \n",
    "NUM_EPOCHS = 3#10 #3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SEQ_LENGTH = 128\n",
    "CLASS_CONFIG = 19 # Choose 19, 6, or 2 based on your experiment\n",
    "RANDOM_STATE = 42\n",
    "SAVE_EVAL_RESULTS = True\n",
    "SAMPLE_SIZE = None # For testing, None=Full Dataset\n",
    "LABEL_COLUMN = 'Attack_Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171a43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 class mapping\n",
    "ATTACK_CATEGORIES_19 = {\n",
    "    'ARP_Spoofing': 'Spoofing',\n",
    "    'MQTT-DDoS-Connect_Flood': 'MQTT-DDoS-Connect_Flood',\n",
    "    'MQTT-DDoS-Publish_Flood': 'MQTT-DDoS-Publish_Flood',\n",
    "    'MQTT-DoS-Connect_Flood': 'MQTT-DoS-Connect_Flood',\n",
    "    'MQTT-DoS-Publish_Flood': 'MQTT-DoS-Publish_Flood',\n",
    "    'MQTT-Malformed_Data': 'MQTT-Malformed_Data',\n",
    "    'Recon-OS_Scan': 'Recon-OS_Scan',\n",
    "    'Recon-Ping_Sweep': 'Recon-Ping_Sweep',\n",
    "    'Recon-Port_Scan': 'Recon-Port_Scan',\n",
    "    'Recon-VulScan': 'Recon-VulScan',\n",
    "    'TCP_IP-DDoS-ICMP': 'DDoS-ICMP',\n",
    "    'TCP_IP-DDoS-SYN': 'DDoS-SYN',\n",
    "    'TCP_IP-DDoS-TCP': 'DDoS-TCP',\n",
    "    'TCP_IP-DDoS-UDP': 'DDoS-UDP',\n",
    "    'TCP_IP-DoS-ICMP': 'DoS-ICMP',\n",
    "    'TCP_IP-DoS-SYN': 'DoS-SYN',\n",
    "    'TCP_IP-DoS-TCP': 'DoS-TCP',\n",
    "    'TCP_IP-DoS-UDP': 'DoS-UDP',\n",
    "    'Benign': 'Benign'\n",
    "}\n",
    "\n",
    "# 6 Class mapping\n",
    "ATTACK_CATEGORIES_6 = { \n",
    "    'Spoofing': 'Spoofing',\n",
    "    'MQTT-DDoS-Connect_Flood': 'MQTT',\n",
    "    'MQTT-DDoS-Publish_Flood': 'MQTT',\n",
    "    'MQTT-DoS-Connect_Flood': 'MQTT',\n",
    "    'MQTT-DoS-Publish_Flood': 'MQTT',\n",
    "    'MQTT-Malformed_Data': 'MQTT',\n",
    "    'Recon-OS_Scan': 'Recon',\n",
    "    'Recon-Ping_Sweep': 'Recon',\n",
    "    'Recon-Port_Scan': 'Recon',\n",
    "    'Recon-VulScan': 'Recon',\n",
    "    'DDoS-ICMP': 'DDoS',\n",
    "    'DDoS-SYN': 'DDoS',\n",
    "    'DDoS-TCP': 'DDoS',\n",
    "    'DDoS-UDP': 'DDoS',\n",
    "    'DoS-ICMP': 'DoS',\n",
    "    'DoS-SYN': 'DoS',\n",
    "    'DoS-TCP': 'DoS',\n",
    "    'DoS-UDP': 'DoS',\n",
    "    'Benign': 'Benign'\n",
    "}\n",
    "\n",
    "# 2 class mapping\n",
    "ATTACK_CATEGORIES_2 = { #\n",
    "    'ARP_Spoofing': 'attack',\n",
    "    'MQTT-DDoS-Connect_Flood': 'attack',\n",
    "    'MQTT-DDoS-Publish_Flood': 'attack',\n",
    "    'MQTT-DoS-Connect_Flood': 'attack',\n",
    "    'MQTT-DoS-Publish_Flood': 'attack',\n",
    "    'MQTT-Malformed_Data': 'attack',\n",
    "    'Recon-OS_Scan': 'attack',\n",
    "    'Recon-Ping_Sweep': 'attack',\n",
    "    'Recon-Port_Scan': 'attack',\n",
    "    'Recon-VulScan': 'attack',\n",
    "    'TCP_IP-DDoS-ICMP': 'attack',\n",
    "    'TCP_IP-DDoS-SYN': 'attack',\n",
    "    'TCP_IP-DDoS-TCP': 'attack',\n",
    "    'TCP_IP-DDoS-UDP': 'attack',\n",
    "    'TCP_IP-DoS-ICMP': 'attack',\n",
    "    'TCP_IP-DoS-SYN': 'attack',\n",
    "    'TCP_IP-DoS-TCP': 'attack',\n",
    "    'TCP_IP-DoS-UDP': 'attack',\n",
    "    'Benign': 'Benign'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257edaf0",
   "metadata": {},
   "source": [
    "# Load Data Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932cc48",
   "metadata": {},
   "source": [
    "## Attack Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0faf2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attack_category(label, class_config):\n",
    "    if class_config == 2:\n",
    "        categories = ATTACK_CATEGORIES_2\n",
    "    elif class_config == 6:\n",
    "        categories = ATTACK_CATEGORIES_6\n",
    "    elif class_config == 19:\n",
    "        categories = ATTACK_CATEGORIES_19\n",
    "        \n",
    "    for key in categories:\n",
    "        if key in label:\n",
    "            return categories[key]\n",
    "    return 'Unknown_Category_From_Filename'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3e0a9",
   "metadata": {},
   "source": [
    "## Textualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1724942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textualize_flow(row, feature_names, sep_token='</s>'):\n",
    "    text_parts = []\n",
    "    for feature_name in feature_names:\n",
    "        if feature_name in row:\n",
    "            value = row[feature_name]\n",
    "            clean_feature_name = feature_name.replace('_',' ').replace('/',' ')\n",
    "            \n",
    "        if pd.isnull(value):\n",
    "            value = 'missing'\n",
    "        elif isinstance(value, float):\n",
    "            value = f'{value:.2f}' if abs(value) >= 0.01 else f'{value:.4f}'\n",
    "        elif isinstance(value, int):\n",
    "            value = str(value)\n",
    "        else:\n",
    "            value = str(value)\n",
    "            \n",
    "        if 'bytes' in clean_feature_name.lower():\n",
    "            text_parts.append(f'The {clean_feature_name} is {value} bytes')\n",
    "        elif 'time' in clean_feature_name.lower() or 'duration' in clean_feature_name.lower():\n",
    "            text_parts.append(f'The {clean_feature_name} is {value} seconds')\n",
    "        else:\n",
    "            text_parts.append(f'The {clean_feature_name} is {value}')\n",
    "    return f' {sep_token}'.join(text_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa623f",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(data_dir, class_config, tokenizer, max_seq_len, text_size_for_val, random_state, sample_size):\n",
    "    logger.info(f'Loading and preparing datasets for {class_config}-class configuration')\n",
    "    \n",
    "    train_path = os.path.join(data_dir, 'train')\n",
    "    test_path = os.path.join(data_dir, 'test')\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.isdir(train_path):\n",
    "        raise FileNotFoundError(f'Training directory not found or is not a directory: {train_path}.')\n",
    "    if not os.path.exists(test_path) or not os.path.isdir(test_path):\n",
    "        raise FileNotFoundError(f'Training directory not found or is not a directory: {test_path}.')\n",
    "        \n",
    "    train_files = [os.path.join(train_path, f) for f in os.listdir(train_path) if f.endswith('.csv')]\n",
    "    test_files = [os.path.join(test_path, f) for f in os.listdir(test_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not train_files:\n",
    "        raise FileNotFoundError(f'No CSV files found in training directory: {train_path}')\n",
    "    if not test_files:\n",
    "        raise FileNotFoundError(f'No CSV files found in training directory: {test_path}')\n",
    "        \n",
    "    df_list_train = [pd.read_csv(f).assign(filename=os.path.basename(f)) for f in train_files]\n",
    "    df_list_test = [pd.read_csv(f).assign(filename=os.path.basename(f)) for f in test_files]\n",
    "    \n",
    "    train_df = pd.concat(df_list_train, ignore_index=True)\n",
    "    test_df = pd.concat(df_list_test, ignore_index=True)\n",
    "    \n",
    "    if sample_size:\n",
    "        logger.info(f'Sampling {sample_size} instances from training data...')\n",
    "        train_df = train_df.sample(n=sample_size, random_state=random_state)\n",
    "        \n",
    "    train_df['Attack_Type_Str'] = train_df['filename'].apply(lambda x: get_attack_category(x, class_config))\n",
    "    test_df['Attack_Type_Str'] = test_df['filename'].apply(lambda x: get_attack_category(x, class_config))\n",
    "    \n",
    "    # Drop rows where Attack_Type could not be determined\n",
    "    train_df = train_df[train_df['Attack_Type_Str'] != 'Unknown_Category_From_Filename'].copy()\n",
    "    test_df = test_df[test_df['Attack_Type_Str'] != 'Unknown_Category_From_Filename'].copy()\n",
    "    \n",
    "    if train_df.empty or test_df.empty:\n",
    "        raise ValueError('No data remaining after filtering for unknown categories. Check filename and category mappings.')\n",
    "        \n",
    "    # Feature column definition\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['filename', 'Attack_Type_Str']]\n",
    "    \n",
    "    # Textualize data\n",
    "    logger.info('Textualizing data...')\n",
    "    \n",
    "    train_df['text'] = train_df.apply(lambda row: textualize_flow(row, feature_cols), axis=1)\n",
    "    test_df['text'] = test_df.apply(lambda row: textualize_flow(row, feature_cols), axis=1)\n",
    "    \n",
    "    # Encoding labels\n",
    "    all_labels = pd.concat([train_df['Attack_Type_Str'], test_df['Attack_Type_Str']]).unique()\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_labels)\n",
    "    train_df['label'] = label_encoder.transform(train_df['Attack_Type_Str'])\n",
    "    test_df['label'] = label_encoder.transform(test_df['Attack_Type_Str'])\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    logger.info(f'Number of classes: {num_classes}, classes: {list(label_encoder.classes_)}')\n",
    "    logger.info(f'Class mapping: {dict(zip(label_encoder.classes_, range(num_classes)))}')\n",
    "    \n",
    "    logger.info(f'Training smaples (before_split): {len(train_df)}')\n",
    "    logger.info(f'Test samples: {len(test_df)}')\n",
    "    \n",
    "    logger.info('Textualized Training Dataset\\n', train_df.head())\n",
    "    logger.info('Textualized Testing Dataset\\n', test_df.head())\n",
    "    \n",
    "    # Splitting training data to create a validation set\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        train_df['text'].tolist(),\n",
    "        train_df['label'].tolist(),\n",
    "        test_size=text_size_for_val,\n",
    "        random_state=random_state,\n",
    "        stratify=train_df['label'].tolist()\n",
    "    )\n",
    "    \n",
    "    test_texts = test_df['text'].tolist()\n",
    "    test_labels = test_df['label'].tolist()\n",
    "    \n",
    "    logger.info(f'Training samples: {len(train_texts)}')\n",
    "    logger.info(f'Validation samples: {len(val_texts)}')\n",
    "    logger.info(f'Test samples: {len(test_texts)}')\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_seq_len)\n",
    "    \n",
    "    train_ds = HFDataset.from_dict({'text': train_texts, 'label': train_labels}).map(tokenize_function, batched=True)\n",
    "    val_ds = HFDataset.from_dict({'text': val_texts, 'label': val_labels}).map(tokenize_function, batched=True)\n",
    "    test_ds = HFDataset.from_dict({'text': test_texts, 'label': test_labels}).map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Calculating class weights\n",
    "    try:\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(train_labels),\n",
    "            y=train_labels\n",
    "        )\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "        logger.info(f'Computed class weights: {class_weights}')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to compute class weights: {e}')\n",
    "        class_weights = {i: 1.0 for i in range(num_classes)}\n",
    "        logger.info(f'Using equal class weights as fallback: {class_weights}')\n",
    "        \n",
    "    return train_ds, val_ds, test_ds, label_encoder, class_weights, feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ff3fa",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec73527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def init_roberta_model(model_name, num_labels, id2label=None, label2id=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Initialize RoBERTa model for sequence classification\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name or path of the pretrained RoBERTa model.\n",
    "        num_labels (int): Number of output labels\n",
    "        id2label (dict, optional): Mapping from label IDs to label names\n",
    "        label2id (dict, optional): Mapping from label names to label IDs\n",
    "        dropout (float, optional): Custom dropout rate for classifier head\n",
    "        \n",
    "    Returns: \n",
    "        RobertaForSequenceClassification: Initialized model\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f\"Initializing RoBERTa model: {model_name} with {num_labels} labels\")\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        hidden_dropout_prob=dropout if dropout is not None else 0.1\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcc8e7",
   "metadata": {},
   "source": [
    "### Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994c6969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainerWithWeightedLoss(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer to apply class weights to the loss function.\n",
    "    \n",
    "    Args: \n",
    "        class_weights (torch.Tensor): Tensor of class weights for imbalanced classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute the weighted cross-entropy loss\n",
    "        \n",
    "        Args:\n",
    "            model: The model being trained\n",
    "            inputs (dict): Input batch including 'labels'\n",
    "            return_output (bool): Whether to return model outputs \n",
    "            \n",
    "        Returns:\n",
    "            loss or (loss, outputs)\n",
    "        \"\"\"\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits#get('logits')\n",
    "\n",
    "        weights_tensor = self.class_weights.to(logits.device) if self.class_weights is not None else None\n",
    "        loss_fnct = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "        loss = loss_fnct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbb2b6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ae79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f15bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196db1d",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd496749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1, \n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656f1d5",
   "metadata": {},
   "source": [
    "# Running..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55545ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading tokenizer for roberta-base...\n",
      "INFO:__main__:Loading and preprocessing data from /data/user/bsindala/PhD/Research/DataSets/CICIoMT2024/WiFI and MQTT/attacks/CSV/...\n",
      "INFO:__main__:Loading and preparing datasets for 19-class configuration\n",
      "INFO:__main__:Textualizing data...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f'Loading tokenizer for {MODEL_NAME}...')\n",
    "        \n",
    "        logger.info(f'Loading and preprocessing data from {DATA_DIR}...')\n",
    "        train_ds, val_ds, test_ds, label_encoder, class_weights, feature_names = load_and_prepare(\n",
    "            data_dir=DATA_DIR, \n",
    "            class_config=CLASS_CONFIG, \n",
    "            tokenizer=tokenizer, \n",
    "            max_seq_len=MAX_SEQ_LENGTH, \n",
    "            text_size_for_val=0.2,\n",
    "            random_state=RANDOM_STATE,\n",
    "            sample_size=SAMPLE_SIZE\n",
    "        )\n",
    "        \n",
    "        logger.info('Sample textualized data:')\n",
    "        for i in range(min(3, len(train_ds))):\n",
    "            logger.info(f\"Text: {train_ds['text'][i]}\")\n",
    "            logger.info(f\"Label: {label_encoder.inverse_transform([train_ds['label'][i]])[0]}\")\n",
    "            \n",
    "        num_labels = len(label_encoder.classes_)\n",
    "        id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        label2id = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "        logger.info(f\"Number of unique labels: {num_labels}\")\n",
    "        logger.info(f\"Training dataset size: {len(train_ds)}\")\n",
    "        logger.info(f\"Validation dataset size: {len(val_ds)}\")\n",
    "        logger.info(f\"Test dataset size: {len(test_ds)}\")\n",
    "        logger.info(f\"Features used for textualization: {feature_names}\")\n",
    "        \n",
    "        logger.info('\\nScript execution completed successfully!')\n",
    "    except Exception as e:\n",
    "        logger.info(f'Error: An exception occured during execution: {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a9154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:.conda-anomalyenv_v310]",
   "language": "python",
   "name": "conda-env-.conda-anomalyenv_v310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
