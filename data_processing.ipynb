{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17f8eaf",
   "metadata": {},
   "source": [
    "# Structure\n",
    "1. Dependecies\n",
    "2. Model\n",
    "3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93fc96",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb92e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-25 09:39:05.005702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756132745.511511   36304 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756132745.630349   36304 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756132746.900082   36304 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756132746.900148   36304 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756132746.900152   36304 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756132746.900154   36304 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-25 09:39:06.923564: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, torch, requests, logging, json, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset as HFDataset\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import RobertaTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddddab33",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b59eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae870a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce16905",
   "metadata": {},
   "source": [
    "# Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad18fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = config['data_dir'] \n",
    "MODEL_NAME = \"roberta-base\"\n",
    "OUTPUT_DIR = config['output']\n",
    "LOGGING_DIR = config['logs'] \n",
    "NUM_EPOCHS = 3#10 #3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SEQ_LENGTH = 128\n",
    "CLASS_CONFIG = 19 # Choose 19, 6, or 2 based on your experiment\n",
    "RANDOM_STATE = 42\n",
    "SAVE_EVAL_RESULTS = True\n",
    "SAMPLE_SIZE = None # For testing, None=Full Dataset\n",
    "LABEL_COLUMN = 'Attack_Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171a43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 class mapping\n",
    "ATTACK_CATEGORIES_19 = {\n",
    "    'ARP_Spoofing': 'Spoofing',\n",
    "    'MQTT-DDoS-Connect_Flood': 'MQTT-DDoS-Connect_Flood',\n",
    "    'MQTT-DDoS-Publish_Flood': 'MQTT-DDoS-Publish_Flood',\n",
    "    'MQTT-DoS-Connect_Flood': 'MQTT-DoS-Connect_Flood',\n",
    "    'MQTT-DoS-Publish_Flood': 'MQTT-DoS-Publish_Flood',\n",
    "    'MQTT-Malformed_Data': 'MQTT-Malformed_Data',\n",
    "    'Recon-OS_Scan': 'Recon-OS_Scan',\n",
    "    'Recon-Ping_Sweep': 'Recon-Ping_Sweep',\n",
    "    'Recon-Port_Scan': 'Recon-Port_Scan',\n",
    "    'Recon-VulScan': 'Recon-VulScan',\n",
    "    'TCP_IP-DDoS-ICMP': 'DDoS-ICMP',\n",
    "    'TCP_IP-DDoS-SYN': 'DDoS-SYN',\n",
    "    'TCP_IP-DDoS-TCP': 'DDoS-TCP',\n",
    "    'TCP_IP-DDoS-UDP': 'DDoS-UDP',\n",
    "    'TCP_IP-DoS-ICMP': 'DoS-ICMP',\n",
    "    'TCP_IP-DoS-SYN': 'DoS-SYN',\n",
    "    'TCP_IP-DoS-TCP': 'DoS-TCP',\n",
    "    'TCP_IP-DoS-UDP': 'DoS-UDP',\n",
    "    'Benign': 'Benign'\n",
    "}\n",
    "\n",
    "# 6 Class mapping\n",
    "ATTACK_CATEGORIES_6 = { \n",
    "    'Spoofing': 'Spoofing',\n",
    "    'MQTT-DDoS-Connect_Flood': 'MQTT',\n",
    "    'MQTT-DDoS-Publish_Flood': 'MQTT',\n",
    "    'MQTT-DoS-Connect_Flood': 'MQTT',\n",
    "    'MQTT-DoS-Publish_Flood': 'MQTT',\n",
    "    'MQTT-Malformed_Data': 'MQTT',\n",
    "    'Recon-OS_Scan': 'Recon',\n",
    "    'Recon-Ping_Sweep': 'Recon',\n",
    "    'Recon-Port_Scan': 'Recon',\n",
    "    'Recon-VulScan': 'Recon',\n",
    "    'DDoS-ICMP': 'DDoS',\n",
    "    'DDoS-SYN': 'DDoS',\n",
    "    'DDoS-TCP': 'DDoS',\n",
    "    'DDoS-UDP': 'DDoS',\n",
    "    'DoS-ICMP': 'DoS',\n",
    "    'DoS-SYN': 'DoS',\n",
    "    'DoS-TCP': 'DoS',\n",
    "    'DoS-UDP': 'DoS',\n",
    "    'Benign': 'Benign'\n",
    "}\n",
    "\n",
    "# 2 class mapping\n",
    "ATTACK_CATEGORIES_2 = { #\n",
    "    'ARP_Spoofing': 'attack',\n",
    "    'MQTT-DDoS-Connect_Flood': 'attack',\n",
    "    'MQTT-DDoS-Publish_Flood': 'attack',\n",
    "    'MQTT-DoS-Connect_Flood': 'attack',\n",
    "    'MQTT-DoS-Publish_Flood': 'attack',\n",
    "    'MQTT-Malformed_Data': 'attack',\n",
    "    'Recon-OS_Scan': 'attack',\n",
    "    'Recon-Ping_Sweep': 'attack',\n",
    "    'Recon-Port_Scan': 'attack',\n",
    "    'Recon-VulScan': 'attack',\n",
    "    'TCP_IP-DDoS-ICMP': 'attack',\n",
    "    'TCP_IP-DDoS-SYN': 'attack',\n",
    "    'TCP_IP-DDoS-TCP': 'attack',\n",
    "    'TCP_IP-DDoS-UDP': 'attack',\n",
    "    'TCP_IP-DoS-ICMP': 'attack',\n",
    "    'TCP_IP-DoS-SYN': 'attack',\n",
    "    'TCP_IP-DoS-TCP': 'attack',\n",
    "    'TCP_IP-DoS-UDP': 'attack',\n",
    "    'Benign': 'Benign'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257edaf0",
   "metadata": {},
   "source": [
    "# Load Data Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932cc48",
   "metadata": {},
   "source": [
    "## Attack Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0faf2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attack_category(label, class_config):\n",
    "    if class_config == 2:\n",
    "        categories = ATTACK_CATEGORIES_2\n",
    "    elif class_config == 6:\n",
    "        categories = ATTACK_CATEGORIES_6\n",
    "    elif class_config == 19:\n",
    "        categories = ATTACK_CATEGORIES_19\n",
    "        \n",
    "    for key in categories:\n",
    "        if key in label:\n",
    "            return categories[key]\n",
    "    return 'Unknown_Category_From_Filename'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3e0a9",
   "metadata": {},
   "source": [
    "## Textualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1724942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textualize_flow(row, feature_names, sep_token='</s>'):\n",
    "    text_parts = []\n",
    "    for feature_name in feature_names:\n",
    "        if feature_name in row:\n",
    "            value = row[feature_name]\n",
    "            clean_feature_name = feature_name.replace('_',' ').replace('/',' ')\n",
    "            \n",
    "        if pd.isnull(value):\n",
    "            value = 'missing'\n",
    "        elif isinstance(value, float):\n",
    "            value = f'{value:.2f}' if abs(value) >= 0.01 else f'{value:.4f}'\n",
    "        elif isinstance(value, int):\n",
    "            value = str(value)\n",
    "        else:\n",
    "            value = str(value)\n",
    "            \n",
    "        if 'bytes' in clean_feature_name.lower():\n",
    "            text_parts.append(f'The {clean_feature_name} is {value} bytes')\n",
    "        elif 'time' in clean_feature_name.lower() or 'duration' in clean_feature_name.lower():\n",
    "            text_parts.append(f'The {clean_feature_name} is {value} seconds')\n",
    "        else:\n",
    "            text_parts.append(f'The {clean_feature_name} is {value}')\n",
    "    return f' {sep_token}'.join(text_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa623f",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(data_dir, class_config, tokenizer, max_seq_len, text_size_for_val, random_state, sample_size):\n",
    "    logger.info(f'Loading and preparing datasets for {class_config}-class configuration')\n",
    "    \n",
    "    train_path = os.path.join(data_dir, 'train')\n",
    "    test_path = os.path.join(data_dir, 'test')\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.isdir(train_path):\n",
    "        raise FileNotFoundError(f'Training directory not found or is not a directory: {train_path}.')\n",
    "    if not os.path.exists(test_path) or not os.path.isdir(test_path):\n",
    "        raise FileNotFoundError(f'Training directory not found or is not a directory: {test_path}.')\n",
    "        \n",
    "    train_files = [os.path.join(train_path, f) for f in os.listdir(train_path) if f.endswith('.csv')]\n",
    "    test_files = [os.path.join(test_path, f) for f in os.listdir(test_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not train_files:\n",
    "        raise FileNotFoundError(f'No CSV files found in training directory: {train_path}')\n",
    "    if not test_files:\n",
    "        raise FileNotFoundError(f'No CSV files found in training directory: {test_path}')\n",
    "        \n",
    "    df_list_train = [pd.read_csv(f).assign(filename=os.path.basename(f)) for f in train_files]\n",
    "    df_list_test = [pd.read_csv(f).assign(filename=os.path.basename(f)) for f in test_files]\n",
    "    \n",
    "    train_df = pd.concat(df_list_train, ignore_index=True)\n",
    "    test_df = pd.concat(df_list_test, ignore_index=True)\n",
    "    \n",
    "    if sample_size:\n",
    "        logger.info(f'Sampling {sample_size} instances from training data...')\n",
    "        train_df = train_df.sample(n=sample_size, random_state=random_state)\n",
    "        \n",
    "    train_df['Attack_Type_Str'] = train_df['filename'].apply(lambda x: get_attack_category(x, class_config))\n",
    "    test_df['Attack_Type_Str'] = test_df['filename'].apply(lambda x: get_attack_category(x, class_config))\n",
    "    \n",
    "    # Drop rows where Attack_Type could not be determined\n",
    "    train_df = train_df[train_df['Attack_Type_Str'] != 'Unknown_Category_From_Filename'].copy()\n",
    "    test_df = test_df[test_df['Attack_Type_Str'] != 'Unknown_Category_From_Filename'].copy()\n",
    "    \n",
    "    if train_df.empty or test_df.empty:\n",
    "        raise ValueError('No data remaining after filtering for unknown categories. Check filename and category mappings.')\n",
    "        \n",
    "    # Feature column definition\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['filename', 'Attack_Type_Str']]\n",
    "    \n",
    "    # Textualize data\n",
    "    logger.info('Textualizing data...')\n",
    "    \n",
    "    train_df['text'] = train_df.apply(lambda row: textualize_flow(row, feature_cols), axis=1)\n",
    "    test_df['text'] = test_df.apply(lambda row: textualize_flow(row, feature_cols), axis=1)\n",
    "    \n",
    "    # Encoding labels\n",
    "    all_labels = pd.concat([train_df['Attack_Type_Str'], test_df['Attack_Type_Str']]).unique()\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_labels)\n",
    "    train_df['label'] = label_encoder.transform(train_df['Attack_Type_Str'])\n",
    "    test_df['label'] = label_encoder.transform(test_df['Attack_Type_Str'])\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    logger.info(f'Number of classes: {num_classes}, classes: {list(label_encoder.classes_)}')\n",
    "    logger.info(f'Class mapping: {dict(zip(label_encoder.classes_, range(num_classes)))}')\n",
    "    \n",
    "    logger.info(f'Training smaples (before_split): {len(train_df)}')\n",
    "    logger.info(f'Test samples: {len(test_df)}')\n",
    "    \n",
    "    logger.info('Textualized Training Dataset\\n', train_df.head())\n",
    "    logger.info('Textualized Testing Dataset\\n', test_df.head())\n",
    "    \n",
    "    # Splitting training data to create a validation set\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        train_df['text'].tolist(),\n",
    "        train_df['label'].tolist(),\n",
    "        test_size=text_size_for_val,\n",
    "        random_state=random_state,\n",
    "        stratify=train_df['label'].tolist()\n",
    "    )\n",
    "    \n",
    "    test_texts = test_df['text'].tolist()\n",
    "    test_labels = test_df['label'].tolist()\n",
    "    \n",
    "    logger.info(f'Training samples: {len(train_texts)}')\n",
    "    logger.info(f'Validation samples: {len(val_texts)}')\n",
    "    logger.info(f'Test samples: {len(test_texts)}')\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_seq_len)\n",
    "    \n",
    "    train_ds = HFDataset.from_dict({'text': train_texts, 'label': train_labels}).map(tokenize_function, batched=True)\n",
    "    val_ds = HFDataset.from_dict({'text': val_texts, 'label': val_labels}).map(tokenize_function, batched=True)\n",
    "    test_ds = HFDataset.from_dict({'text': test_texts, 'label': test_labels}).map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Calculating class weights\n",
    "    try:\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(train_labels),\n",
    "            y=train_labels\n",
    "        )\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "        logger.info(f'Computed class weights: {class_weights}')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to compute class weights: {e}')\n",
    "        class_weights = {i: 1.0 for i in range(num_classes)}\n",
    "        logger.info(f'Using equal class weights as fallback: {class_weights}')\n",
    "        \n",
    "    return train_ds, val_ds, test_ds, label_encoder, class_weights, feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ff3fa",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec73527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def init_roberta_model(model_name, num_labels, id2label=None, label2id=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Initialize RoBERTa model for sequence classification\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name or path of the pretrained RoBERTa model.\n",
    "        num_labels (int): Number of output labels\n",
    "        id2label (dict, optional): Mapping from label IDs to label names\n",
    "        label2id (dict, optional): Mapping from label names to label IDs\n",
    "        dropout (float, optional): Custom dropout rate for classifier head\n",
    "        \n",
    "    Returns: \n",
    "        RobertaForSequenceClassification: Initialized model\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f\"Initializing RoBERTa model: {model_name} with {num_labels} labels\")\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        hidden_dropout_prob=dropout if dropout is not None else 0.1\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcc8e7",
   "metadata": {},
   "source": [
    "### Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994c6969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainerWithWeightedLoss(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer to apply class weights to the loss function.\n",
    "    \n",
    "    Args: \n",
    "        class_weights (torch.Tensor): Tensor of class weights for imbalanced classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute the weighted cross-entropy loss\n",
    "        \n",
    "        Args:\n",
    "            model: The model being trained\n",
    "            inputs (dict): Input batch including 'labels'\n",
    "            return_output (bool): Whether to return model outputs \n",
    "            \n",
    "        Returns:\n",
    "            loss or (loss, outputs)\n",
    "        \"\"\"\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits#get('logits')\n",
    "\n",
    "        weights_tensor = self.class_weights.to(logits.device) if self.class_weights is not None else None\n",
    "        loss_fnct = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "        loss = loss_fnct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbb2b6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ae79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f15bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196db1d",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd496749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1, \n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656f1d5",
   "metadata": {},
   "source": [
    "# Running..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c55545ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading tokenizer for roberta-base...\n",
      "INFO:__main__:Loading and preprocessing data from /data/user/bsindala/PhD/Research/DataSets/CICIoMT2024/WiFI and MQTT/attacks/CSV/...\n",
      "INFO:__main__:Loading and preparing datasets for 19-class configuration\n",
      "INFO:__main__:Textualizing data...\n",
      "INFO:__main__:Number of classes: 19, classes: ['Benign', 'DDoS-ICMP', 'DDoS-SYN', 'DDoS-TCP', 'DDoS-UDP', 'DoS-ICMP', 'DoS-SYN', 'DoS-TCP', 'DoS-UDP', 'MQTT-DDoS-Connect_Flood', 'MQTT-DDoS-Publish_Flood', 'MQTT-DoS-Connect_Flood', 'MQTT-DoS-Publish_Flood', 'MQTT-Malformed_Data', 'Recon-OS_Scan', 'Recon-Ping_Sweep', 'Recon-Port_Scan', 'Recon-VulScan', 'Spoofing']\n",
      "INFO:__main__:Class mapping: {'Benign': 0, 'DDoS-ICMP': 1, 'DDoS-SYN': 2, 'DDoS-TCP': 3, 'DDoS-UDP': 4, 'DoS-ICMP': 5, 'DoS-SYN': 6, 'DoS-TCP': 7, 'DoS-UDP': 8, 'MQTT-DDoS-Connect_Flood': 9, 'MQTT-DDoS-Publish_Flood': 10, 'MQTT-DoS-Connect_Flood': 11, 'MQTT-DoS-Publish_Flood': 12, 'MQTT-Malformed_Data': 13, 'Recon-OS_Scan': 14, 'Recon-Ping_Sweep': 15, 'Recon-Port_Scan': 16, 'Recon-VulScan': 17, 'Spoofing': 18}\n",
      "INFO:__main__:Training smaples (before_split): 7160831\n",
      "INFO:__main__:Test samples: 1614182\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/scratch/local/ipykernel_36304/3043886449.py\", line 6, in <module>\n",
      "    train_ds, val_ds, test_ds, label_encoder, class_weights, feature_names = load_and_prepare(\n",
      "  File \"/scratch/local/ipykernel_36304/1460217040.py\", line 64, in load_and_prepare\n",
      "    logger.info('Textualized Training Dataset\\n', train_df.head())\n",
      "Message: 'Textualized Training Dataset\\n'\n",
      "Arguments: (   Header_Length  Protocol Type  Duration          Rate         Srate  Drate  \\\n",
      "0          114.6            5.9      51.2  30655.032896  30655.032896    0.0   \n",
      "1          129.0            6.0      64.0  90366.618129  90366.618129    0.0   \n",
      "2          321.1            7.1     100.6  13324.032213  13324.032213    0.0   \n",
      "3          292.6            6.0      80.8      3.897624      3.897624    0.0   \n",
      "4          483.6            7.1      69.9      8.191751      8.191751    0.0   \n",
      "\n",
      "   fin_flag_number  syn_flag_number  rst_flag_number  psh_flag_number  \\\n",
      "0              0.0              0.0              0.0              0.5   \n",
      "1              0.0              0.0              0.0              0.5   \n",
      "2              0.0              0.0              0.0              0.4   \n",
      "3              0.0              0.0              0.0              0.6   \n",
      "4              0.0              0.0              0.0              0.4   \n",
      "\n",
      "   ack_flag_number  ece_flag_number  cwr_flag_number  ack_count  syn_count  \\\n",
      "0              0.7              0.0              0.0        0.0        0.0   \n",
      "1              1.0              0.0              0.0        0.0        0.0   \n",
      "2              0.9              0.0              0.0        0.0        0.0   \n",
      "3              1.0              0.0              0.0        0.0        0.0   \n",
      "4              0.9              0.0              0.0        0.0        0.0   \n",
      "\n",
      "   fin_count  rst_count  HTTP  HTTPS  DNS  Telnet  SMTP  SSH  IRC  TCP  UDP  \\\n",
      "0        0.0        1.0   0.0    0.0  0.0     0.0   0.0  0.0  0.0  0.7  0.1   \n",
      "1        0.0        1.5   0.0    0.0  0.0     0.0   0.0  0.0  0.0  1.0  0.0   \n",
      "2        0.0        2.6   0.0    0.0  0.0     0.0   0.0  0.0  0.0  0.9  0.1   \n",
      "3        0.0        3.4   0.0    0.0  0.0     0.0   0.0  0.0  0.0  1.0  0.0   \n",
      "4        0.0        4.1   0.0    0.3  0.0     0.0   0.0  0.0  0.0  0.9  0.1   \n",
      "\n",
      "   DHCP  ARP  ICMP  IGMP  IPv  LLC  Tot sum   Min    Max         AVG  \\\n",
      "0   0.0  0.2   0.0   0.0  0.8  0.8    568.0  42.0  181.4   95.151429   \n",
      "1   0.0  0.0   0.0   0.0  1.0  1.0   1438.2  42.0  214.0   93.121933   \n",
      "2   0.0  0.0   0.0   0.0  1.0  1.0    617.2  63.6  187.5  102.235437   \n",
      "3   0.0  0.0   0.0   0.0  1.0  1.0   1693.4  54.0  246.0  110.225159   \n",
      "4   0.0  0.0   0.0   0.0  1.0  1.0    913.8  66.0  590.4  144.389087   \n",
      "\n",
      "          Std  Tot size           IAT  Number   Magnitue      Radius  \\\n",
      "0   54.909851      99.6  1.694104e+08     5.5  13.639579   77.654256   \n",
      "1   49.319359      81.2  1.694104e+08    13.5  13.647331   69.908824   \n",
      "2   48.063324     119.4  8.832250e-02     5.5  14.179865   67.971805   \n",
      "3   59.841524      87.5  1.694104e+08    13.5  14.849839   84.803609   \n",
      "4  180.179746     160.9  8.218880e-02     5.5  16.531551  254.812640   \n",
      "\n",
      "     Covariance  Variance  Weight                      filename  \\\n",
      "0   3685.081162       0.9    38.5  Recon-OS_Scan_train.pcap.csv   \n",
      "1   2457.350159       1.0   244.6  Recon-OS_Scan_train.pcap.csv   \n",
      "2   3322.481708       0.7    38.5  Recon-OS_Scan_train.pcap.csv   \n",
      "3   3609.775298       1.0   244.6  Recon-OS_Scan_train.pcap.csv   \n",
      "4  53155.729481       0.9    38.5  Recon-OS_Scan_train.pcap.csv   \n",
      "\n",
      "  Attack_Type_Str                                               text  label  \n",
      "0   Recon-OS_Scan  The Header Length is 114.60 </s>The Protocol T...     14  \n",
      "1   Recon-OS_Scan  The Header Length is 129.00 </s>The Protocol T...     14  \n",
      "2   Recon-OS_Scan  The Header Length is 321.10 </s>The Protocol T...     14  \n",
      "3   Recon-OS_Scan  The Header Length is 292.60 </s>The Protocol T...     14  \n",
      "4   Recon-OS_Scan  The Header Length is 483.60 </s>The Protocol T...     14  ,)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/bsindala/.conda/envs/anomalyenv_v310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/scratch/local/ipykernel_36304/3043886449.py\", line 6, in <module>\n",
      "    train_ds, val_ds, test_ds, label_encoder, class_weights, feature_names = load_and_prepare(\n",
      "  File \"/scratch/local/ipykernel_36304/1460217040.py\", line 65, in load_and_prepare\n",
      "    logger.info('Textualized Testing Dataset\\n', test_df.head())\n",
      "Message: 'Textualized Testing Dataset\\n'\n",
      "Arguments: (   Header_Length  Protocol Type  Duration  Rate  Srate  Drate  \\\n",
      "0           54.0            6.0      64.0   0.0    0.0    0.0   \n",
      "1           54.0            6.0      64.0   0.0    0.0    0.0   \n",
      "2           54.0            6.0      64.0   0.0    0.0    0.0   \n",
      "3           54.0            6.0      64.0   0.0    0.0    0.0   \n",
      "4           54.0            6.0      64.0   0.0    0.0    0.0   \n",
      "\n",
      "   fin_flag_number  syn_flag_number  rst_flag_number  psh_flag_number  \\\n",
      "0              0.0              0.0              0.0              0.0   \n",
      "1              0.0              0.0              0.0              0.0   \n",
      "2              0.0              0.0              0.0              0.0   \n",
      "3              0.0              0.0              0.0              0.0   \n",
      "4              0.0              0.0              0.0              0.0   \n",
      "\n",
      "   ack_flag_number  ece_flag_number  cwr_flag_number  ack_count  syn_count  \\\n",
      "0              0.0              0.0              0.0        0.0        0.0   \n",
      "1              0.0              0.0              0.0        0.0        0.0   \n",
      "2              0.0              0.0              0.0        0.0        0.0   \n",
      "3              0.0              0.0              0.0        0.0        0.0   \n",
      "4              0.0              0.0              0.0        0.0        0.0   \n",
      "\n",
      "   fin_count  rst_count  HTTP  HTTPS  DNS  Telnet  SMTP  SSH  IRC  TCP  UDP  \\\n",
      "0        0.0        0.0   0.0    0.0  0.0     0.0   0.0  0.0  0.0  1.0  0.0   \n",
      "1        0.0        0.0   0.0    0.0  0.0     0.0   0.0  0.0  0.0  1.0  0.0   \n",
      "2        0.0        0.0   0.0    0.0  0.0     0.0   0.0  0.0  0.0  1.0  0.0   \n",
      "3        0.0        0.0   0.0    0.0  0.0     0.0   0.0  0.0  0.0  1.0  0.0   \n",
      "4        0.0        0.0   0.0    0.0  0.0     0.0   0.0  0.0  0.0  1.0  0.0   \n",
      "\n",
      "   DHCP  ARP  ICMP  IGMP  IPv  LLC  Tot sum   Min   Max   AVG  Std  Tot size  \\\n",
      "0   0.0  0.0   0.0   0.0  1.0  1.0    567.0  54.0  54.0  54.0  0.0      54.0   \n",
      "1   0.0  0.0   0.0   0.0  1.0  1.0    567.0  54.0  54.0  54.0  0.0      54.0   \n",
      "2   0.0  0.0   0.0   0.0  1.0  1.0    567.0  54.0  54.0  54.0  0.0      54.0   \n",
      "3   0.0  0.0   0.0   0.0  1.0  1.0    567.0  54.0  54.0  54.0  0.0      54.0   \n",
      "4   0.0  0.0   0.0   0.0  1.0  1.0    567.0  54.0  54.0  54.0  0.0      54.0   \n",
      "\n",
      "            IAT  Number   Magnitue  Radius  Covariance  Variance  Weight  \\\n",
      "0  1.016354e+08     9.5  10.392305     0.0         0.0       0.0  141.55   \n",
      "1  8.469615e+07     9.5  10.392305     0.0         0.0       0.0  141.55   \n",
      "2  8.469615e+07     9.5  10.392305     0.0         0.0       0.0  141.55   \n",
      "3  8.469615e+07     9.5  10.392305     0.0         0.0       0.0  141.55   \n",
      "4  8.469615e+07     9.5  10.392305     0.0         0.0       0.0  141.55   \n",
      "\n",
      "                        filename Attack_Type_Str  \\\n",
      "0  TCP_IP-DDoS-TCP_test.pcap.csv        DDoS-TCP   \n",
      "1  TCP_IP-DDoS-TCP_test.pcap.csv        DDoS-TCP   \n",
      "2  TCP_IP-DDoS-TCP_test.pcap.csv        DDoS-TCP   \n",
      "3  TCP_IP-DDoS-TCP_test.pcap.csv        DDoS-TCP   \n",
      "4  TCP_IP-DDoS-TCP_test.pcap.csv        DDoS-TCP   \n",
      "\n",
      "                                                text  label  \n",
      "0  The Header Length is 54.00 </s>The Protocol Ty...      3  \n",
      "1  The Header Length is 54.00 </s>The Protocol Ty...      3  \n",
      "2  The Header Length is 54.00 </s>The Protocol Ty...      3  \n",
      "3  The Header Length is 54.00 </s>The Protocol Ty...      3  \n",
      "4  The Header Length is 54.00 </s>The Protocol Ty...      3  ,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training samples: 5728664\n",
      "INFO:__main__:Validation samples: 1432167\n",
      "INFO:__main__:Test samples: 1614182\n",
      "Map: 100%|██████████| 5728664/5728664 [1:09:07<00:00, 1381.10 examples/s]\n",
      "Map: 100%|██████████| 1432167/1432167 [16:41<00:00, 1430.04 examples/s]\n",
      "Map: 100%|██████████| 1614182/1614182 [18:32<00:00, 1451.02 examples/s]\n",
      "INFO:__main__:Computed class weights: {0: np.float64(1.9554864357266377), 1: np.float64(0.24513275536691004), 2: np.float64(0.46995511251158856), 3: np.float64(0.46849246328141586), 4: np.float64(0.23037644770371102), 5: np.float64(0.9053416075252223), 6: np.float64(0.8528709149047227), 7: np.float64(0.9908041273416234), 8: np.float64(0.6647601895646604), 9: np.float64(2.178074186615141), 10: np.float64(13.644159271379644), 11: np.float64(29.5075975317036), 12: np.float64(8.492961651191441), 13: np.float64(73.46701549194624), 14: np.float64(22.390363254043322), 15: np.float64(509.30512091038406), 16: np.float64(4.4877373160519065), 17: np.float64(173.48022530434255), 18: np.float64(23.48563885176409)}\n",
      "INFO:__main__:Sample textualized data:\n",
      "INFO:__main__:Text: The Header Length is 26107.00 </s>The Protocol Type is 17.00 </s>The Duration is 64.00 seconds </s>The Rate is 22644.95 </s>The Srate is 22644.95 </s>The Drate is 0.0000 </s>The fin flag number is 0.0000 </s>The syn flag number is 0.0000 </s>The rst flag number is 0.0000 </s>The psh flag number is 0.0000 </s>The ack flag number is 0.0000 </s>The ece flag number is 0.0000 </s>The cwr flag number is 0.0000 </s>The ack count is 0.0000 </s>The syn count is 0.0000 </s>The fin count is 0.0000 </s>The rst count is 0.0000 </s>The HTTP is 0.0000 </s>The HTTPS is 0.0000 </s>The DNS is 0.0000 </s>The Telnet is 0.0000 </s>The SMTP is 0.0000 </s>The SSH is 0.0000 </s>The IRC is 0.0000 </s>The TCP is 0.0000 </s>The UDP is 1.00 </s>The DHCP is 0.0000 </s>The ARP is 0.0000 </s>The ICMP is 0.0000 </s>The IGMP is 0.0000 </s>The IPv is 1.00 </s>The LLC is 1.00 </s>The Tot sum is 525.00 </s>The Min is 50.00 </s>The Max is 50.00 </s>The AVG is 50.00 </s>The Std is 0.0000 </s>The Tot size is 50.00 </s>The IAT is 84696589.59 </s>The Number is 9.50 </s>The Magnitue is 10.00 </s>The Radius is 0.0000 </s>The Covariance is 0.0000 </s>The Variance is 0.0000 </s>The Weight is 141.55\n",
      "INFO:__main__:Label: DDoS-UDP\n",
      "INFO:__main__:Text: The Header Length is 21867.84 </s>The Protocol Type is 17.00 </s>The Duration is 64.00 seconds </s>The Rate is 15329.73 </s>The Srate is 15329.73 </s>The Drate is 0.0000 </s>The fin flag number is 0.0000 </s>The syn flag number is 0.0000 </s>The rst flag number is 0.0000 </s>The psh flag number is 0.0000 </s>The ack flag number is 0.0000 </s>The ece flag number is 0.0000 </s>The cwr flag number is 0.0000 </s>The ack count is 0.0000 </s>The syn count is 0.0000 </s>The fin count is 0.0000 </s>The rst count is 0.0000 </s>The HTTP is 0.0000 </s>The HTTPS is 0.0000 </s>The DNS is 0.0000 </s>The Telnet is 0.0000 </s>The SMTP is 0.0000 </s>The SSH is 0.0000 </s>The IRC is 0.0000 </s>The TCP is 0.0000 </s>The UDP is 1.00 </s>The DHCP is 0.0000 </s>The ARP is 0.0000 </s>The ICMP is 0.0000 </s>The IGMP is 0.0000 </s>The IPv is 1.00 </s>The LLC is 1.00 </s>The Tot sum is 528.92 </s>The Min is 50.00 </s>The Max is 53.92 </s>The AVG is 50.23 </s>The Std is 0.93 </s>The Tot size is 50.56 </s>The IAT is 84696738.61 </s>The Number is 9.50 </s>The Magnitue is 10.02 </s>The Radius is 1.31 </s>The Covariance is 12.38 </s>The Variance is 0.07 </s>The Weight is 141.55\n",
      "INFO:__main__:Label: DDoS-UDP\n",
      "INFO:__main__:Text: The Header Length is 0.0000 </s>The Protocol Type is 1.00 </s>The Duration is 64.00 seconds </s>The Rate is 45.53 </s>The Srate is 45.53 </s>The Drate is 0.0000 </s>The fin flag number is 0.0000 </s>The syn flag number is 0.0000 </s>The rst flag number is 0.0000 </s>The psh flag number is 0.0000 </s>The ack flag number is 0.0000 </s>The ece flag number is 0.0000 </s>The cwr flag number is 0.0000 </s>The ack count is 0.0000 </s>The syn count is 0.0000 </s>The fin count is 0.0000 </s>The rst count is 0.0000 </s>The HTTP is 0.0000 </s>The HTTPS is 0.0000 </s>The DNS is 0.0000 </s>The Telnet is 0.0000 </s>The SMTP is 0.0000 </s>The SSH is 0.0000 </s>The IRC is 0.0000 </s>The TCP is 0.0000 </s>The UDP is 0.0000 </s>The DHCP is 0.0000 </s>The ARP is 0.0000 </s>The ICMP is 1.00 </s>The IGMP is 0.0000 </s>The IPv is 1.00 </s>The LLC is 1.00 </s>The Tot sum is 441.00 </s>The Min is 42.00 </s>The Max is 42.00 </s>The AVG is 42.00 </s>The Std is 0.0000 </s>The Tot size is 42.00 </s>The IAT is 84679269.63 </s>The Number is 9.50 </s>The Magnitue is 9.17 </s>The Radius is 0.0000 </s>The Covariance is 0.0000 </s>The Variance is 0.0000 </s>The Weight is 141.55\n",
      "INFO:__main__:Label: DoS-ICMP\n",
      "INFO:__main__:Number of unique labels: 19\n",
      "INFO:__main__:Training dataset size: 5728664\n",
      "INFO:__main__:Validation dataset size: 1432167\n",
      "INFO:__main__:Test dataset size: 1614182\n",
      "INFO:__main__:Features used for textualization: ['Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight']\n",
      "INFO:__main__:\n",
      "Script execution completed successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f'Loading tokenizer for {MODEL_NAME}...')\n",
    "        \n",
    "        logger.info(f'Loading and preprocessing data from {DATA_DIR}...')\n",
    "        train_ds, val_ds, test_ds, label_encoder, class_weights, feature_names = load_and_prepare(\n",
    "            data_dir=DATA_DIR, \n",
    "            class_config=CLASS_CONFIG, \n",
    "            tokenizer=tokenizer, \n",
    "            max_seq_len=MAX_SEQ_LENGTH, \n",
    "            text_size_for_val=0.2,\n",
    "            random_state=RANDOM_STATE,\n",
    "            sample_size=SAMPLE_SIZE\n",
    "        )\n",
    "        \n",
    "        logger.info('Sample textualized data:')\n",
    "        for i in range(min(3, len(train_ds))):\n",
    "            logger.info(f\"Text: {train_ds['text'][i]}\")\n",
    "            logger.info(f\"Label: {label_encoder.inverse_transform([train_ds['label'][i]])[0]}\")\n",
    "            \n",
    "        num_labels = len(label_encoder.classes_)\n",
    "        id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        label2id = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "        logger.info(f\"Number of unique labels: {num_labels}\")\n",
    "        logger.info(f\"Training dataset size: {len(train_ds)}\")\n",
    "        logger.info(f\"Validation dataset size: {len(val_ds)}\")\n",
    "        logger.info(f\"Test dataset size: {len(test_ds)}\")\n",
    "        logger.info(f\"Features used for textualization: {feature_names}\")\n",
    "        \n",
    "        logger.info('\\nScript execution completed successfully!')\n",
    "    except Exception as e:\n",
    "        logger.info(f'Error: An exception occured during execution: {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a9154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:.conda-anomalyenv_v310]",
   "language": "python",
   "name": "conda-env-.conda-anomalyenv_v310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
